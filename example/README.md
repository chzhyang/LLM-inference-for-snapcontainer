# LLM server examples on Intel CPU

This folder contains examples of running LLM inference server on Intel CPU, including

- [intel/openvino](./openvino/llama2/README.md): run llama2-7b(INT8) inference with openvino
- [intel/ipex](./ipex): run opt-1.3b(BF16) inference with ipex

