# LLM server examples on Intel CPU

This folder contains examples of running LLM inference server on Intel CPU, including

- [intel/openvino](./openvino/llama2/README.md): run llama2-7b(BF16, INT8) inference on openvino
- [huggingface/transformers](./hf_transformers): run Hugging Face Transformers model on transformers
- [intel/bigdl-llm/transformers](./bigdl_llm_transformers/README.md): run chatglm2-6b and llama2-7b on bigdl-llm/transformers
- [intel/bigdl-llm/cpp](./bigdl_llm_cpp/README.md): run llama2-7b of INT4 on bigdl-llm/cpp

