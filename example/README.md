# LLM server examples on Intel CPU

This folder contains examples of running LLM inference server on Intel CPU, including

- [intel/openvino](./openvino/llama2/README.md): run llama2-7b(BF16, INT8) inference on openvino
- [huggingface/transformers](./hf_transformers): run Hugging Face Transformers model on transformers

