# OpenVINO examples

LLM server support running llama2 and chatglm2 inference on Intel/OpenVINO

This folder contains examples of running LLM inference server with Intel/OpenVINO on Intel CPU:

- [Llama2](./llama2)
